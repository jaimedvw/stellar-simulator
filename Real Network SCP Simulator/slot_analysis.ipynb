{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.venv/lib/python3.11/site-packages (from pandas) (1.25.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:06:40.677957Z",
     "start_time": "2025-06-08T13:06:38.610236Z"
    }
   },
   "id": "69be3bd04b03062e",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_transaction_count(line):\n",
    "    pattern = r\"transactions = \\{([^}]+)\\}\"\n",
    "    match = re.search(pattern, line)\n",
    "    if match:\n",
    "        return set(re.findall(r\"Transaction ([a-fA-F0-9]+)\", match.group(1)))\n",
    "    return set()\n",
    "\n",
    "def get_timestamp(line):\n",
    "    pattern = r\"^\\d+\\.\\d+\"\n",
    "    match = re.match(pattern, line)\n",
    "    return float(match.group(0)) if match else None\n",
    "\n",
    "def get_node_name(line):\n",
    "    pattern = r\"Node ([A-Za-z0-9]+)\"\n",
    "    match = re.search(pattern, line)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def count_unique_mempool_transactions(file_path, node_number):\n",
    "    unique_transactions = set()\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if f\"Node {node_number}\" in line and \"from mempool\" in line:\n",
    "                unique_transactions.update(re.findall(r\"Transaction ([a-fA-F0-9]+)\", line))\n",
    "    return len(unique_transactions)\n",
    "\n",
    "def extract_slot(message_line):\n",
    "    \"\"\"\n",
    "    Extracts the slot number from an externalize message log line.\n",
    "    Looks for the pattern \"slot <number>\".\n",
    "    \"\"\"\n",
    "    pattern = r\"slot (\\d+)\"\n",
    "    match = re.search(pattern, message_line)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def process_log_lines(file_path):\n",
    "    \"\"\"\n",
    "    Extracts **all** SCPExternalize messages per node and stores their relevant details.\n",
    "    A new column 'Slot' is added by parsing the slot number from the log message.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Only consider lines that contain relevant externalization messages\n",
    "            if (\"appended SCPExternalize message for slot\" not in line and\n",
    "                \"adopting externalized value for slot\" not in line):\n",
    "                continue\n",
    "            \n",
    "            node_name = get_node_name(line)\n",
    "            timestamp = get_timestamp(line)\n",
    "            transactions = get_transaction_count(line)\n",
    "            slot = extract_slot(line)  # Extract slot directly from the log line\n",
    "\n",
    "            if node_name:\n",
    "                data.append({\n",
    "                    \"node name\": node_name,\n",
    "                    \"Timestamp of finalisation\": timestamp,\n",
    "                    \"Finalised transactions\": transactions,\n",
    "                    \"Externalize message\": line.strip(),\n",
    "                    \"Slot\": slot\n",
    "                })\n",
    "    \n",
    "    # Convert the collected data to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # If any row didn't have a slot parsed, you might want to drop or handle it:\n",
    "    df = df.dropna(subset=[\"Slot\"])\n",
    "\n",
    "    # Count the number of finalized transactions for each externalize message\n",
    "    df[\"No. of finalised transactions\"] = df[\"Finalised transactions\"].apply(len)\n",
    "    \n",
    "    # Compute total transactions for each node from mempool logs\n",
    "    df[\"total_transactions\"] = df[\"node name\"].apply(lambda node: count_unique_mempool_transactions(file_path, node))\n",
    "    \n",
    "    # Calculate number of transactions not finalized for each node\n",
    "    df[\"no. of transactions not finalised\"] = df[\"total_transactions\"] - df[\"No. of finalised transactions\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# file_path = 'path/to/your/simulator_events_log.txt'\n",
    "# df = process_log_lines(file_path)\n",
    "# print(df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:06:40.707634Z",
     "start_time": "2025-06-08T13:06:40.696879Z"
    }
   },
   "id": "e5426146017cc83d",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                      node name                       Timestamp of finalisation               Finalised transactions                              Externalize message                  Slot  No. of finalised transactions  total_transactions  no. of transactions not finalised\n0  GA5IMBV5AMJ6VAORQ6XOUNDMEMAS34DSKL5O6RSRWL6LR6...           2.4000            {c3ba851d, ea0f2551, cb32994b, e4318a4e, b8cff...  2.40 - NODE - INFO - Node GA5IMBV5AMJ6VAORQ6XO...    1                 7                         0                         -7                ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: center;\">\n      <th></th>\n      <th>node name</th>\n      <th>Timestamp of finalisation</th>\n      <th>Finalised transactions</th>\n      <th>Externalize message</th>\n      <th>Slot</th>\n      <th>No. of finalised transactions</th>\n      <th>total_transactions</th>\n      <th>no. of transactions not finalised</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>GA5IMBV5AMJ6VAORQ6XOUNDMEMAS34DSKL5O6RSRWL6LR6...</td>\n      <td>2.4000</td>\n      <td>{c3ba851d, ea0f2551, cb32994b, e4318a4e, b8cff...</td>\n      <td>2.40 - NODE - INFO - Node GA5IMBV5AMJ6VAORQ6XO...</td>\n      <td>1</td>\n      <td>7</td>\n      <td>0</td>\n      <td>-7</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#file_path = ('scripts/logs/run_1/simulator_events_log.txt')\n",
    "file_path = ('src/simulator_events_log.txt')\n",
    "df = process_log_lines(file_path)\n",
    "\n",
    "# df_sorted = df.sort_values(by='Timestamp of finalisation', ascending=True)\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', 1000)  # Prevent line wrapping\n",
    "pd.set_option('display.colheader_justify', 'center')  # Center column headers\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)  # Format float values\n",
    "\n",
    "df = df.sort_values(by='Slot', ascending=True)\n",
    "\n",
    "display(df) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:06:40.745425Z",
     "start_time": "2025-06-08T13:06:40.702813Z"
    }
   },
   "id": "241e6b522d963ec3",
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Analyze Transaction Matches across slots"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e669312cdca1d43"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions that appear in more than one slot:\n"
     ]
    }
   ],
   "source": [
    "def analyze_transaction_matches(df):\n",
    "    tx_occurrences = {}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        node = row['node name']\n",
    "        slot = row['Slot']\n",
    "        # row['Finalised transactions'] is a set; iterate through each transaction hash\n",
    "        for tx in row['Finalised transactions']:\n",
    "            if tx not in tx_occurrences:\n",
    "                tx_occurrences[tx] = set()\n",
    "            tx_occurrences[tx].add((node, slot))\n",
    "    \n",
    "    duplicates = {tx: occ for tx, occ in tx_occurrences.items() if len(occ) > 1} \n",
    "\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "duplicates = analyze_transaction_matches(df)\n",
    "print(\"Transactions that appear in more than one slot:\")\n",
    "for tx, occ in duplicates.items():\n",
    "    print(f\"Transaction {tx} appears in: {sorted(list(occ))}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:06:40.755429Z",
     "start_time": "2025-06-08T13:06:40.751732Z"
    }
   },
   "id": "5aa4ba4566cd37c5",
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SHOW ALL TRANSACTIONS THAT APPEAR IN MULTIPLE SLOTS FOR THE SAME NODE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e097ab3688539d38"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-node transaction sets by slot:\n",
      "Node GA5IMBV5AMJ6VAORQ6XOUNDMEMAS34DSKL5O6RSRWL6LR6F7EAZY5MB4:\n",
      "  Slot 1: ['6bc309db', 'b8cff9ce', 'c204bed9', 'c3ba851d', 'cb32994b', 'e4318a4e', 'ea0f2551']\n",
      "\n",
      "Transactions that appear for the same node in multiple slots:\n"
     ]
    }
   ],
   "source": [
    "def analyze_node_slots_and_multislot_occurrences(df):\n",
    "    \"\"\"\n",
    "    Analyzes the DataFrame to extract:\n",
    "      - For each node, a dictionary mapping each Slot to the set of finalized transaction hashes \n",
    "        that were reported in that slot.\n",
    "      - For each node, a dictionary mapping transaction hashes (that occur in multiple slots) to the\n",
    "        set of slot numbers where they appear.\n",
    "    \n",
    "    Args:\n",
    "        df (pandas.DataFrame): A DataFrame with at least the columns:\n",
    "            - 'node name'\n",
    "            - 'Slot'\n",
    "            - 'Finalised transactions' (a set of transaction hashes)\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (node_slots, multi_occurrences) where:\n",
    "          - node_slots is a dict: { node_name: { slot: set(transaction hashes), ... }, ... }\n",
    "          - multi_occurrences is a dict: { node_name: { transaction_hash: set(slots) where it appears, ... }, ... }\n",
    "    \"\"\"\n",
    "    # Build a dictionary that records for each node which transactions are finalized in each slot.\n",
    "    node_slots = {}\n",
    "    for idx, row in df.iterrows():\n",
    "        node = row['node name']\n",
    "        slot = row['Slot']\n",
    "        txs = row['Finalised transactions']  # expected to be a set of transaction hashes\n",
    "        if node not in node_slots:\n",
    "            node_slots[node] = {}\n",
    "        if slot not in node_slots[node]:\n",
    "            node_slots[node][slot] = set()\n",
    "        node_slots[node][slot].update(txs)\n",
    "    \n",
    "    # Now, for each node, gather transaction occurrences across slots.\n",
    "    multi_occurrences = {}\n",
    "    for node, slot_dict in node_slots.items():\n",
    "        tx_occurrences = {}  # maps tx -> set of slots where it occurs\n",
    "        for slot, txs in slot_dict.items():\n",
    "            for tx in txs:\n",
    "                if tx not in tx_occurrences:\n",
    "                    tx_occurrences[tx] = set()\n",
    "                tx_occurrences[tx].add(slot)\n",
    "        # Filter to only transactions that appear in more than one slot.\n",
    "        multi = {tx: slots for tx, slots in tx_occurrences.items() if len(slots) > 1}\n",
    "        if multi:\n",
    "            multi_occurrences[node] = multi\n",
    "\n",
    "    return node_slots, multi_occurrences\n",
    "\n",
    "# Example usage:\n",
    "node_slots, multi_occurrences = analyze_node_slots_and_multislot_occurrences(df)\n",
    "\n",
    "print(\"Per-node transaction sets by slot:\")\n",
    "for node, slot_info in node_slots.items():\n",
    "    print(f\"Node {node}:\")\n",
    "    for slot, txs in sorted(slot_info.items()):\n",
    "        print(f\"  Slot {slot}: {sorted(list(txs))}\")\n",
    "\n",
    "print(\"\\nTransactions that appear for the same node in multiple slots:\")\n",
    "for node, tx_info in multi_occurrences.items():\n",
    "    print(f\"Node {node}:\")\n",
    "    for tx, slots in tx_info.items():\n",
    "        print(f\"  Transaction {tx} appears in slots: {sorted(list(slots))}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:06:40.794643Z",
     "start_time": "2025-06-08T13:06:40.763213Z"
    }
   },
   "id": "6600c7c4c2ff4b76",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions that appear in more than one slot, with their first‐3‐word log types:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def analyze_transaction_matches_with_msgs(df):\n",
    "    \"\"\"\n",
    "    Find transactions that show up in more than one slot *and* record\n",
    "    the first 3 words of the log messages that caused each appearance.\n",
    "    \n",
    "    Returns a dict:\n",
    "      tx_hash -> {\n",
    "         'occurrences': set of (node, slot),\n",
    "         'msg_types':   set of first‑3‑word summaries\n",
    "      }\n",
    "    \"\"\"\n",
    "    tx_occurrences = {}   # tx -> set((node,slot))\n",
    "    tx_msgtypes   = {}    # tx -> set(msg_type)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        node    = row['node name']\n",
    "        slot    = row['Slot']\n",
    "        txs     = row['Finalised transactions']\n",
    "        msg     = row['Externalize message']\n",
    "        # grab the first 3 words of the log line\n",
    "        msg_type = \" \".join(msg.split()[:10])\n",
    "        \n",
    "        for tx in txs:\n",
    "            tx_occurrences.setdefault(tx, set()).add((node, slot))\n",
    "            tx_msgtypes.setdefault(tx,   set()).add(msg_type)\n",
    "    \n",
    "    # now filter to only those that appear in >1 slot\n",
    "    duplicates = {}\n",
    "    for tx, occ in tx_occurrences.items():\n",
    "        if len(occ) > 1:\n",
    "            duplicates[tx] = {\n",
    "                'occurrences': occ,\n",
    "                'msg_types':   tx_msgtypes.get(tx, set())\n",
    "            }\n",
    "    return duplicates\n",
    "\n",
    "# --- example usage ---\n",
    "duplicates = analyze_transaction_matches_with_msgs(df)\n",
    "\n",
    "print(\"Transactions that appear in more than one slot, with their first‐3‐word log types:\")\n",
    "for tx, info in duplicates.items():\n",
    "    occ       = sorted(info['occurrences'])\n",
    "    msg_types = sorted(info['msg_types'])\n",
    "    print(f\"- {tx}:\")\n",
    "    print(f\"    slots:    {occ}\")\n",
    "    print(f\"    messages: {msg_types}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:06:40.818997Z",
     "start_time": "2025-06-08T13:06:40.772291Z"
    }
   },
   "id": "2ca9475427f2ea3c",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def analyze_node_repeated_txs_with_msgs(df):\n",
    "    \"\"\"\n",
    "    For each node, find transactions that are finalized\n",
    "    in more than one slot.  For each such tx, record the\n",
    "    slot *and* the first‑3‑words of the log message that\n",
    "    reported it in that slot.\n",
    "    \n",
    "    Returns a dict:\n",
    "      node -> {\n",
    "        tx_hash -> [ (slot, msg_type), ... ]\n",
    "      }\n",
    "    \"\"\"\n",
    "    # intermediate store: node -> tx -> list of (slot,msg_type)\n",
    "    temp = {}\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        node = row['node name']\n",
    "        slot = row['Slot']\n",
    "        txs  = row['Finalised transactions']\n",
    "        msg  = row['Externalize message']\n",
    "        # first three words of the message\n",
    "        msg_type = \" \".join(msg.split()[:50])\n",
    "        \n",
    "        # initialize per‐node\n",
    "        if node not in temp:\n",
    "            temp[node] = {}\n",
    "        \n",
    "        for tx in txs:\n",
    "            temp[node].setdefault(tx, []).append((slot, msg_type))\n",
    "    \n",
    "    # now filter to only those txes that appear in >1 distinct slot for each node\n",
    "    result = {}\n",
    "    for node, tx_dict in temp.items():\n",
    "        repeated = {}\n",
    "        for tx, slot_msgs in tx_dict.items():\n",
    "            # collect distinct slots\n",
    "            slots = { sm[0] for sm in slot_msgs }\n",
    "            if len(slots) > 1:\n",
    "                # keep all occurrences (slot,msg_type)\n",
    "                repeated[tx] = slot_msgs\n",
    "        if repeated:\n",
    "            result[node] = repeated\n",
    "    \n",
    "    return result\n",
    "\n",
    "# --- example usage ---\n",
    "multi = analyze_node_repeated_txs_with_msgs(df)\n",
    "\n",
    "for node, tx_info in multi.items():\n",
    "    print(f\"\\nNode {node}:\")\n",
    "    for tx, occurrences in tx_info.items():\n",
    "        print(f\"  Transaction {tx!r} is in multiple slots:\")\n",
    "        for slot, msg_type in sorted(occurrences):\n",
    "            print(f\"    Slot {slot:>2} via “{msg_type}”\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:06:40.819980Z",
     "start_time": "2025-06-08T13:06:40.781902Z"
    }
   },
   "id": "4d6666d1e92b14dd",
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analyse duplicates within transactions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "350943860a42b6a4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates within each transaction set per slot:\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates_within_slots(df):\n",
    "    duplicates_within = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        node = row['node name']\n",
    "        slot = row['Slot']\n",
    "        tx_list = list(row['Finalised transactions'])\n",
    "        seen = set()\n",
    "        duplicates = set()\n",
    "\n",
    "        for tx in tx_list:\n",
    "            if tx in seen:\n",
    "                duplicates.add(tx)\n",
    "            else:\n",
    "                seen.add(tx)\n",
    "\n",
    "        if duplicates:\n",
    "            duplicates_within.append({\n",
    "                'node': node,\n",
    "                'slot': slot,\n",
    "                'duplicates': list(duplicates)\n",
    "            })\n",
    "\n",
    "    return duplicates_within\n",
    "\n",
    "duplicates_within_slots = check_duplicates_within_slots(df)\n",
    "print(\"Duplicates within each transaction set per slot:\")\n",
    "for entry in duplicates_within_slots:\n",
    "    print(f\"Node: {entry['node']}, Slot: {entry['slot']}, Duplicates: {entry['duplicates']}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:06:40.820725Z",
     "start_time": "2025-06-08T13:06:40.787857Z"
    }
   },
   "id": "cc9f63dfdc3fcb3d",
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "ADD INTERLEDGER CHECKS"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8c23cf3429014b5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inter-Ledger Agreement Time: nan\n"
     ]
    }
   ],
   "source": [
    "def calculate_inter_ledger_agreement_time(df):\n",
    "    df = df.sort_values(by='Timestamp of finalisation')\n",
    "    time_diffs = df['Timestamp of finalisation'].diff().dropna()\n",
    "    \n",
    "    return time_diffs.mean()\n",
    "\n",
    "avg_time = calculate_inter_ledger_agreement_time(df)\n",
    "print(f\"Average Inter-Ledger Agreement Time: {avg_time}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:06:40.879888Z",
     "start_time": "2025-06-08T13:06:40.794985Z"
    }
   },
   "id": "4f6a1ea63b72affc",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['sequence number'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[60], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m final_experiment_df \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msequence number\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTimestamp of finalisation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mNo. of finalised transactions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mno. of transactions not finalised\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m      6\u001B[0m \u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m      8\u001B[0m display(final_experiment_df)\n",
      "File \u001B[0;32m~/PycharmProjects/thesis-stellar-simulator/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   4106\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[1;32m   4107\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[0;32m-> 4108\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m   4110\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[1;32m   4111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/thesis-stellar-simulator/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[0;34m(self, key, axis_name)\u001B[0m\n\u001B[1;32m   6197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   6198\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[0;32m-> 6200\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6202\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[1;32m   6203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[1;32m   6204\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/thesis-stellar-simulator/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[0;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[1;32m   6249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6251\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[0;32m-> 6252\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"['sequence number'] not in index\""
     ]
    }
   ],
   "source": [
    "final_experiment_df = df[[\n",
    "    \"sequence number\",\n",
    "    \"Timestamp of finalisation\",\n",
    "    \"No. of finalised transactions\",\n",
    "    \"no. of transactions not finalised\"\n",
    "]]\n",
    "\n",
    "display(final_experiment_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:06:41.043646Z",
     "start_time": "2025-06-08T13:06:40.806076Z"
    }
   },
   "id": "18a0b7d813327d2f",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "avg_difference = (final_experiment_df[\"no. of transactions not finalised\"] - final_experiment_df[\"No. of finalised transactions\"]).mean()\n",
    "\n",
    "print(f\"Average difference: {avg_difference}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-08T13:06:40.862913Z"
    }
   },
   "id": "a183fd7400e7d199",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "avg_finalised = final_experiment_df[\"No. of finalised transactions\"].mean()\n",
    "avg_total = (final_experiment_df[\"No. of finalised transactions\"] + \n",
    "             final_experiment_df[\"no. of transactions not finalised\"]).mean()\n",
    "\n",
    "finalised_percentage = (avg_finalised / avg_total) * 100 if avg_total != 0 else 0\n",
    "\n",
    "print(f\"Percentage of finalised transactions vs total: {finalised_percentage:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-08T13:06:40.866497Z"
    }
   },
   "id": "4632d0b4bbcc90a4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "58caaddc0d13d7be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
