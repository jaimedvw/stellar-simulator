{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-15T16:03:17.816560Z",
     "start_time": "2025-06-15T16:03:17.813788Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "SUMMARY_CSV = \"simulation_summary.csv\"\n",
    "FIELDNAMES = [\n",
    "    \"node_count\",\n",
    "    \"simulation_time\",\n",
    "    \"sim_params\",\n",
    "    \"total_tx_created\",\n",
    "    \"total_slots\",\n",
    "    \"total_tx_in_all_slots\",\n",
    "    \"avg_txs_per_slot\",\n",
    "    \"avg_inter_slot_time\",\n",
    "    \"all_tests_passed\",\n",
    "]\n",
    "\n",
    "def append_summary_row(row: dict):\n",
    "    os.makedirs(os.path.dirname(SUMMARY_CSV) or \".\", exist_ok=True)\n",
    "    write_header = not os.path.isfile(SUMMARY_CSV)\n",
    "    with open(SUMMARY_CSV, \"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=FIELDNAMES)\n",
    "        if write_header:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T16:03:17.827953Z",
     "start_time": "2025-06-15T16:03:17.823073Z"
    }
   },
   "id": "ab056275d697efb0",
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def get_transaction_count(line):\n",
    "    pattern = r\"transactions = \\{([^}]+)\\}\"\n",
    "    match = re.search(pattern, line)\n",
    "    if match:\n",
    "        return set(re.findall(r\"Transaction ([a-fA-F0-9]+)\", match.group(1)))\n",
    "    return set()\n",
    "\n",
    "def get_timestamp(line):\n",
    "    pattern = r\"^\\d+\\.\\d+\"\n",
    "    match = re.match(pattern, line)\n",
    "    return float(match.group(0)) if match else None\n",
    "\n",
    "def get_node_name(line):\n",
    "    pattern = r\"Node ([A-Z0-9]+)\"\n",
    "    match = re.search(pattern, line)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "\n",
    "def process_log_lines(file_path):\n",
    "    node_data = defaultdict(lambda: {\n",
    "        \"Timestamp of finalisation\": None,\n",
    "        \"Finalised transactions\": set(),\n",
    "        \"Externalize messages\": []\n",
    "    })\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    for line in lines:\n",
    "        if ('appended SCPExternalize message' not in line\n",
    "                and 'adopting externalized value for slot' not in line):\n",
    "            continue\n",
    "        node_name = get_node_name(line)\n",
    "        timestamp = get_timestamp(line)\n",
    "        transactions = get_transaction_count(line)\n",
    "        if node_name:\n",
    "            if node_data[node_name][\"Timestamp of finalisation\"] is None:\n",
    "                node_data[node_name][\"Timestamp of finalisation\"] = timestamp\n",
    "            node_data[node_name][\"Finalised transactions\"].update(transactions)\n",
    "            node_data[node_name][\"Externalize messages\"].append(line.strip())\n",
    "    df = pd.DataFrame.from_dict(node_data, orient='index')\n",
    "    df.index.name = \"sequence number\"\n",
    "    df = df.reset_index()\n",
    "    df[\"No. of finalised transactions\"] = df[\"Finalised transactions\"].apply(len)\n",
    "    return df\n",
    "\n",
    "def extract_slot_finalisation_times(file_path):\n",
    "    slot_times = {}\n",
    "    pattern = re.compile(r\"(\\d+\\.\\d+).*?Node [A-Z0-9]+.*?(?:appended|adopting) externalize.*?slot (\\d+)\", re.IGNORECASE)\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            m = pattern.search(line)\n",
    "            if m:\n",
    "                timestamp = float(m.group(1))\n",
    "                slot = int(m.group(2))\n",
    "                # Only record the first externalize seen for each slot\n",
    "                if slot not in slot_times:\n",
    "                    slot_times[slot] = timestamp\n",
    "    # Return sorted list of finalisation times by slot number\n",
    "    return [slot_times[slot] for slot in sorted(slot_times)]\n",
    "\n",
    "\n",
    "def compute_summary_metrics(events_log_path: str):\n",
    "    mined_hashes = set()\n",
    "    mining_pat = re.compile(r\"\\[Transaction ([A-Fa-f0-9]+) time = [\\d\\.]+\\] mined to the mempool!\")\n",
    "    with open(events_log_path, 'r') as f:\n",
    "        for line in f:\n",
    "            m = mining_pat.search(line)\n",
    "            if m:\n",
    "                mined_hashes.add(m.group(1))\n",
    "    total_tx_created = len(mined_hashes)\n",
    "    df = process_log_lines(events_log_path)\n",
    "    total_slots = df[\"Externalize messages\"].apply(len).sum()\n",
    "    all_finalized = set()\n",
    "    for s in df[\"Finalised transactions\"]:\n",
    "        all_finalized.update(s)\n",
    "    total_tx_in_all_slots = len(all_finalized)\n",
    "    avg_txs_per_slot = (total_tx_in_all_slots / total_slots) if total_slots else 0.0\n",
    "    \n",
    "    \n",
    "    slot_finalisation_times = extract_slot_finalisation_times(events_log_path)\n",
    "    intervals = [t2 - t1 for t1, t2 in zip(slot_finalisation_times, slot_finalisation_times[1:])]\n",
    "    avg_inter_slot_time = (sum(intervals) / len(intervals)) if intervals else 0.0\n",
    "\n",
    "\n",
    "    return (\n",
    "        total_tx_created,\n",
    "        total_slots,\n",
    "        total_tx_in_all_slots,\n",
    "        avg_txs_per_slot,\n",
    "        avg_inter_slot_time\n",
    "    )\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T16:03:17.838373Z",
     "start_time": "2025-06-15T16:03:17.833856Z"
    }
   },
   "id": "638f03807b124205",
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EDIT THE \"row\" below with appropriate parameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44b6a29a3b1e4ef0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Assume you already have the functions defined as above\n",
    "log_file = \"Runs for Tx creation scaling /ER_Topology 30% threshold/050/simulator_events_log.txt\"\n",
    "\n",
    "# Get your computed metrics\n",
    "(total_tx_created,\n",
    " total_slots,\n",
    " total_tx_in_all_slots,\n",
    " avg_txs_per_slot,\n",
    " avg_inter_slot_time) = compute_summary_metrics(log_file)\n",
    "\n",
    "row = {\n",
    "    \"node_count\": 50,  # Set appropriately for your log\n",
    "    \"simulation_time\": 100,  # Set appropriately for your log\n",
    "    \"sim_params\": '{\"n_nodes\": 50, \"sim_duration\": 100, \"mine: 0.}',\n",
    "    \"total_tx_created\": total_tx_created,\n",
    "    \"total_slots\": total_slots,\n",
    "    \"total_tx_in_all_slots\": total_tx_in_all_slots,\n",
    "    \"avg_txs_per_slot\": f\"{avg_txs_per_slot:.2f}\",\n",
    "    \"avg_inter_slot_time\": f\"{avg_inter_slot_time:.2f}\",\n",
    "    \"all_tests_passed\": True,  # Or set according to your logic\n",
    "}\n",
    "append_summary_row(row)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-15T16:03:17.961435Z",
     "start_time": "2025-06-15T16:03:17.839717Z"
    }
   },
   "id": "bb1052ed3dd056c",
   "execution_count": 96
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
